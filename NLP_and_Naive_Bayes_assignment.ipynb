{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### **Text Classification Using Naive Bayes and Sentiment Analysis  on Blog Posts**\n",
        "\n",
        "\n",
        "\n",
        "**Objective**:\n",
        "\n",
        "This project requires to develop a text classification model with the help of the Naive Bayes algorithm that classifies blog posts from the “blogs_categories.csv” dataset and further performs sentiment analysis to identify the mood (positive, negative, neutral). This task is designed to expand my proficiency in the areas of text classification, sentiment analysis, and the use of NLP techniques, which is consistent with the objective of producing a detailed and properly documented report.\n",
        "\n",
        "**Dataset**:\n",
        "\n",
        "The \"blogs_categories.csv\" dataset includes blog posts with associated categories. Key columns are:\n",
        "\n",
        "**Data**: Contains the blog post text.\n",
        "\n",
        "**Labels**: Indicates the category (e.g., politics, sports, tech).  \n",
        "\n",
        "\n",
        "\n",
        "**Details**: The dataset contains roughly 3,000 records (deduced from the sample), which provide a varied basis for multi-class classification and sentiment analysis"
      ],
      "metadata": {
        "id": "apxHP152oaBY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1. Data Exploration and Preprocessing**\n",
        "\n",
        "\n",
        "\n",
        "**1.1 Loading the \"blogs_categories.csv\" Dataset and Perform an Exploratory Data Analysis to Understand Its Structure and Content**:\n",
        "\n",
        "As the first step, I procured the dataset and conducted a preliminary examination to grasp its outline and the included data."
      ],
      "metadata": {
        "id": "p1bVfdjnoxE7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the dataset\n",
        "data = pd.read_csv('blogs.csv')\n",
        "\n",
        "# Basic exploration\n",
        "print(\"Dataset Shape:\", data.shape)\n",
        "print(\"First 5 Rows:\\n\", data.head())\n",
        "print(\"Missing Values:\\n\", data.isnull().sum())\n",
        "print(\"Data Types:\\n\", data.dtypes)\n",
        "print(\"Unique Labels:\", data['Labels'].nunique())\n",
        "print(\"Label Distribution:\\n\", data['Labels'].value_counts())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "diI3eeLBo0Zl",
        "outputId": "ce0043b4-a01b-4e26-b661-145fe799beb4"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset Shape: (2000, 2)\n",
            "First 5 Rows:\n",
            "                                                 Data       Labels\n",
            "0  Path: cantaloupe.srv.cs.cmu.edu!magnesium.club...  alt.atheism\n",
            "1  Newsgroups: alt.atheism\\nPath: cantaloupe.srv....  alt.atheism\n",
            "2  Path: cantaloupe.srv.cs.cmu.edu!das-news.harva...  alt.atheism\n",
            "3  Path: cantaloupe.srv.cs.cmu.edu!magnesium.club...  alt.atheism\n",
            "4  Xref: cantaloupe.srv.cs.cmu.edu alt.atheism:53...  alt.atheism\n",
            "Missing Values:\n",
            " Data      0\n",
            "Labels    0\n",
            "dtype: int64\n",
            "Data Types:\n",
            " Data      object\n",
            "Labels    object\n",
            "dtype: object\n",
            "Unique Labels: 20\n",
            "Label Distribution:\n",
            " Labels\n",
            "alt.atheism                 100\n",
            "comp.graphics               100\n",
            "comp.os.ms-windows.misc     100\n",
            "comp.sys.ibm.pc.hardware    100\n",
            "comp.sys.mac.hardware       100\n",
            "comp.windows.x              100\n",
            "misc.forsale                100\n",
            "rec.autos                   100\n",
            "rec.motorcycles             100\n",
            "rec.sport.baseball          100\n",
            "rec.sport.hockey            100\n",
            "sci.crypt                   100\n",
            "sci.electronics             100\n",
            "sci.med                     100\n",
            "sci.space                   100\n",
            "soc.religion.christian      100\n",
            "talk.politics.guns          100\n",
            "talk.politics.mideast       100\n",
            "talk.politics.misc          100\n",
            "talk.religion.misc          100\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The dataset comprises 2000 rows and 2 columns. It is complete (text and labels) and has no missing values. There are 20 different classes in total, and each class contains 100 examples, i.e., the distribution is even. Moreover, certain texts are of the type that contain the headers (e.g., \"Path\", \"Xref\") that have been mentioned to require a more detailed cleaning process."
      ],
      "metadata": {
        "id": "7IrOmYkZpQOG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1.2 Preprocess the Data by Cleaning the Text (Removing Punctuation, Converting to Lowercase, etc.), Tokenizing, and Removing Stopwords:**"
      ],
      "metadata": {
        "id": "3HiEAIB9pV3m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "\n",
        "# Loading the dataset from the uploaded file\n",
        "data = pd.read_csv(\"blogs.csv\")\n",
        "\n",
        "# Displaying initial information to understand structure\n",
        "print(\"Dataset loaded successfully.\")\n",
        "print(\"Shape of dataset:\", data.shape)\n",
        "print(\"\\nColumns available:\\n\", data.columns)\n",
        "\n",
        "# Checking first few entries\n",
        "print(\"\\nSample data:\\n\", data.head())\n",
        "\n",
        "# Defining a custom list of common English stopwords\n",
        "stop_words = set([\n",
        "    \"a\", \"an\", \"the\", \"and\", \"or\", \"but\", \"if\", \"while\", \"with\", \"to\",\n",
        "    \"of\", \"at\", \"by\", \"for\", \"on\", \"in\", \"out\", \"up\", \"down\", \"from\",\n",
        "    \"into\", \"over\", \"under\", \"again\", \"further\", \"then\", \"once\", \"here\",\n",
        "    \"there\", \"all\", \"any\", \"both\", \"each\", \"few\", \"more\", \"most\", \"other\",\n",
        "    \"some\", \"such\", \"no\", \"nor\", \"not\", \"only\", \"own\", \"same\", \"so\",\n",
        "    \"than\", \"too\", \"very\", \"can\", \"will\", \"just\", \"don\", \"should\", \"now\"\n",
        "])\n",
        "\n",
        "# Function for text preprocessing\n",
        "def preprocess_text(text):\n",
        "    # Handling missing or non-text values\n",
        "    if not isinstance(text, str):\n",
        "        return ''\n",
        "    # Converting text to lowercase for consistency\n",
        "    text = text.lower()\n",
        "    # Removing all characters except alphabets and spaces\n",
        "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
        "    # Splitting text into tokens\n",
        "    tokens = text.split()\n",
        "    # Removing stopwords\n",
        "    tokens = [word for word in tokens if word not in stop_words]\n",
        "    # Rejoining tokens into a cleaned string\n",
        "    return ' '.join(tokens)\n",
        "\n",
        "# Identifying the text column for cleaning\n",
        "# You can adjust this if your text is under a different column name\n",
        "text_column = 'Data' if 'Data' in data.columns else data.columns[0]\n",
        "\n",
        "# Applying preprocessing on the selected text column\n",
        "# Removing punctuation, numbers, and stopwords\n",
        "data['cleaned_text'] = data[text_column].apply(preprocess_text)\n",
        "\n",
        "# Displaying sample cleaned text entries\n",
        "print(\"\\nFirst 5 cleaned text entries:\\n\")\n",
        "print(data[['cleaned_text']].head())\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v-e49X8OpYLo",
        "outputId": "c107d6e7-2cab-4463-aedf-ed0e2826dc01"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset loaded successfully.\n",
            "Shape of dataset: (2000, 2)\n",
            "\n",
            "Columns available:\n",
            " Index(['Data', 'Labels'], dtype='object')\n",
            "\n",
            "Sample data:\n",
            "                                                 Data       Labels\n",
            "0  Path: cantaloupe.srv.cs.cmu.edu!magnesium.club...  alt.atheism\n",
            "1  Newsgroups: alt.atheism\\nPath: cantaloupe.srv....  alt.atheism\n",
            "2  Path: cantaloupe.srv.cs.cmu.edu!das-news.harva...  alt.atheism\n",
            "3  Path: cantaloupe.srv.cs.cmu.edu!magnesium.club...  alt.atheism\n",
            "4  Xref: cantaloupe.srv.cs.cmu.edu alt.atheism:53...  alt.atheism\n",
            "\n",
            "First 5 cleaned text entries:\n",
            "\n",
            "                                        cleaned_text\n",
            "0  path cantaloupesrvcscmuedumagnesiumclubcccmued...\n",
            "1  newsgroups altatheism path cantaloupesrvcscmue...\n",
            "2  path cantaloupesrvcscmuedudasnewsharvardedunoc...\n",
            "3  path cantaloupesrvcscmuedumagnesiumclubcccmued...\n",
            "4  xref cantaloupesrvcscmuedu altatheism talkreli...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the preprocessing stage, the text data was converted entirely to lowercase for consistency, and all punctuation marks, special symbols, and numeric characters were removed to eliminate noise. The text was then tokenized into individual words, and common English stopwords were filtered out to retain only meaningful content.\n",
        "\n",
        "The first few cleaned text samples (as shown in the output) demonstrate that unnecessary components such as headers, email paths, and repetitive metadata have been significantly reduced. Words like “Path,” “Newsgroups,” and similar header-related terms have been stripped of formatting and reduced to plain, meaningful tokens.\n",
        "\n",
        "This cleaning process effectively minimizes redundancy and prepares the text for feature extraction and model training, ensuring that only relevant linguistic patterns are retained for classification tasks."
      ],
      "metadata": {
        "id": "JQN98qnsK3hN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1.3 Perform Feature Extraction to Convert Text Data into a Format that Can Be Used by the Naive Bayes Model, Using Techniques Such as TF-IDF**:  \n",
        "\n",
        "TF-IDF (Term Frequency-Inverse Document Frequency) was chosen to be implemented on the cleaned text in order to transform the text into a numerical format that the Naive Bayes model can process."
      ],
      "metadata": {
        "id": "EyLC3ENYK8ud"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# Initialized TF-IDF vectorizer\n",
        "tfidf = TfidfVectorizer(max_features=5000, stop_words='english')\n",
        "\n",
        "# Fitting and transforming the cleaned text\n",
        "X_tfidf = tfidf.fit_transform(data['cleaned_text'])\n",
        "\n",
        "# Converting labels to a list\n",
        "y = data['Labels'].values\n",
        "\n",
        "print(\"TF-IDF Matrix Shape:\", X_tfidf.shape)\n",
        "print(\"Sample of TF-IDF Features (first 5 terms):\", list(tfidf.get_feature_names_out())[:5])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k2o4IiTKLd_3",
        "outputId": "25593504-f5a2-40a0-9697-26dd1b4c4c60"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TF-IDF Matrix Shape: (2000, 5000)\n",
            "Sample of TF-IDF Features (first 5 terms): ['aa', 'aafreenetcarletonca', 'aaron', 'ab', 'abate']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The TF-IDF matrix depicts (2000, 5000) where 2000 stands for the number of documents and 5000 features (limited by max_features) are the dimensions of the matrix. The example features probably indicate that the vectorizer has chosen the terms 'aa' and 'aaron' which might be some domain-specific or rare words because of very little stopword filtering besides the default one. This matrix is now combined with the Naive Bayes classification."
      ],
      "metadata": {
        "id": "C30y2WWKLr89"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2. Naive Bayes Model for Text Classification**\n",
        "\n",
        "\n",
        "\n",
        "**2.1 Split the Data into Training and Test Sets**:\n",
        "\n",
        "To evaluate how well the model works, the data set has been divided into the training and test sets."
      ],
      "metadata": {
        "id": "0J7tHOcZLufv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Split the data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_tfidf, y, test_size=0.2, random_state=42)\n",
        "\n",
        "print(\"Training Set Shape:\", X_train.shape)\n",
        "print(\"Test Set Shape:\", X_test.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qrY8sFFfLyH3",
        "outputId": "dbeeb08b-3ea4-42c7-b3de-e3f333cf788c"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Set Shape: (1600, 5000)\n",
            "Test Set Shape: (400, 5000)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data gathering was separated: 80% (1,600 samples) was assigned to training, and 20% (400 samples) to testing. This distribution ensures that both a training set and a validation set are representative. The parameter random_state=42 is used in order to get the same results every time."
      ],
      "metadata": {
        "id": "H0i1-D0YL_Mf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2.2 Implement a Naive Bayes Classifier to Categorize the Blog Posts into Their Respective Categories**  \n",
        "\n",
        "I implemented the Multinomial Naive Bayes classifier, which is the best one for text processing with TF-IDF features."
      ],
      "metadata": {
        "id": "8bsPwwTBMB6i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.naive_bayes import MultinomialNB\n",
        "\n",
        "# Initialize the Naive Bayes classifier\n",
        "nb_classifier = MultinomialNB()\n",
        "\n",
        "# Training the model\n",
        "nb_classifier.fit(X_train, y_train)\n",
        "\n",
        "print(\"Model Training Completed\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PyVjpjy2METD",
        "outputId": "336e605a-784e-4128-c3e1-de58caa7bb9c"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Training Completed\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The training was done using the parameters of the TF-IDF features of the training set and the corresponding labels. The method's feature independence assumption is being made here, which is quite successful for text data."
      ],
      "metadata": {
        "id": "dRkM-QuVMKOb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2.3 Train the Model on the Training Set and Make Predictions on the Test Set**   \n",
        "\n",
        "Performed the training of the model, and its predictions were used to evaluate the accuracy for the test set."
      ],
      "metadata": {
        "id": "2RVcqDfqMN-E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Making predictions\n",
        "y_pred = nb_classifier.predict(X_test)\n",
        "\n",
        "# Displaying first 5 predictions\n",
        "print(\"First 5 Predictions:\", y_pred[:5])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6c_q584xMQy5",
        "outputId": "e6559202-1054-497d-e29b-a92a86480400"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First 5 Predictions: ['talk.politics.misc' 'comp.sys.ibm.pc.hardware' 'sci.med'\n",
            " 'rec.sport.baseball' 'sci.electronics']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The first five test samples, for which the model predicted categories, contained a variety of labels, such as \"talk.politics.misc\" and \"sci.electronics\". The next step will be to verify these predictions for their correctness."
      ],
      "metadata": {
        "id": "XfK_E68HMa7p"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3. Sentiment Analysis**\n",
        "\n",
        "**3.1 Choose a Suitable Library or Method for Performing Sentiment Analysis on the Blog Post Texts**  \n",
        "\n",
        "For sentiment analysis, I have decided on the TextBlob library. It is a simplistic yet efficient tool for a user to understand the mood of a given text by a polarity value it assigns (-1 to 1). Here, a negative number reveals the negative sentiment, a positive one shows positive sentiment, and a score that is nearly zero is interpreted as neutral sentiment."
      ],
      "metadata": {
        "id": "nvUx7NDcMgs6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from textblob import TextBlob\n",
        "\n",
        "# Function to get sentiment polarity\n",
        "def get_sentiment(text):\n",
        "    analysis = TextBlob(text)\n",
        "    polarity = analysis.sentiment.polarity\n",
        "    if polarity > 0:\n",
        "        return 'positive'\n",
        "    elif polarity < 0:\n",
        "        return 'negative'\n",
        "    else:\n",
        "        return 'neutral'\n",
        "\n",
        "# Apply sentiment analysis to the original 'Data' column\n",
        "data['sentiment'] = data['Data'].apply(get_sentiment)\n",
        "\n",
        "print(\"First 5 Sentiments:\\n\", data['sentiment'].head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ogJGYXKBMkwY",
        "outputId": "739525e2-25fe-48d8-b0bc-2bfe6054eab5"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First 5 Sentiments:\n",
            " 0    positive\n",
            "1    negative\n",
            "2    positive\n",
            "3    positive\n",
            "4    positive\n",
            "Name: sentiment, dtype: object\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "TextBlob library was called into action to check out the mood of the blog posts by analyzing the original 'Data' column. The first five records give a small set of different moods, where the first samples can be considered as having been mixed emotionally, with happy (4) and sad (1) sentiments being identified"
      ],
      "metadata": {
        "id": "R6WCV9r-Mpah"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3.2 Analyze the Sentiments Expressed in the Blog Posts and Categorize Them as Positive, Negative, or Neutral  \n",
        "\n",
        "All blog posts were analyzed for sentiment and categorized according to that."
      ],
      "metadata": {
        "id": "afOyiUbGMu7W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Count sentiment distribution\n",
        "sentiment_distribution = data['sentiment'].value_counts()\n",
        "\n",
        "print(\"Sentiment Distribution:\\n\", sentiment_distribution)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O9S5Ay59Mv4M",
        "outputId": "0b4a2992-e36d-4f65-92ff-c146981d6b59"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sentiment Distribution:\n",
            " sentiment\n",
            "positive    1543\n",
            "negative     457\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "The breakdown shows that 77.15% of the posts (1,543) are positive and 22.85% (457) are negative, with no neutral sentiments found. This indicates that the dataset is biased towards positivity, perhaps because of the type of the blog content or TextBlob's being more responsive to positive signals."
      ],
      "metadata": {
        "id": "6JhYr8PfMyp8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3.3 Examine the Distribution of Sentiments Across Different Categories and Summarize Your Findings**  \n",
        "\n",
        "Explored the variations emotive in the 20 categories by group analysis."
      ],
      "metadata": {
        "id": "ehDHtEcMM2WG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Group by category and sentiment\n",
        "sentiment_by_category = data.groupby(['Labels', 'sentiment']).size().unstack(fill_value=0)\n",
        "\n",
        "print(\"Sentiment Distribution by Category:\\n\", sentiment_by_category)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K2RjM4agM5Te",
        "outputId": "43d576d7-ee7f-4bc6-d0f8-182ec5992de7"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sentiment Distribution by Category:\n",
            " sentiment                 negative  positive\n",
            "Labels                                      \n",
            "alt.atheism                     23        77\n",
            "comp.graphics                   24        76\n",
            "comp.os.ms-windows.misc         22        78\n",
            "comp.sys.ibm.pc.hardware        20        80\n",
            "comp.sys.mac.hardware           24        76\n",
            "comp.windows.x                  27        73\n",
            "misc.forsale                    16        84\n",
            "rec.autos                       17        83\n",
            "rec.motorcycles                 26        74\n",
            "rec.sport.baseball              29        71\n",
            "rec.sport.hockey                34        66\n",
            "sci.crypt                       19        81\n",
            "sci.electronics                 19        81\n",
            "sci.med                         29        71\n",
            "sci.space                       27        73\n",
            "soc.religion.christian          13        87\n",
            "talk.politics.guns              30        70\n",
            "talk.politics.mideast           22        78\n",
            "talk.politics.misc              22        78\n",
            "talk.religion.misc              14        86\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "It is found from the analysis that sentiments associations with the different categories fluctuate. The categories 'soc.religion.christian' (87% positive) and 'talk.religion.misc' (86% positive) give victorious aspects of the sentiment spectrum, which may be interpreted as the result of the uplifting content these areas provide. 'misc.forsale' (84% positive) and 'rec.autos' (83% positive) also show signs of the positive side, which can be attributed to the general excitement reflected in these categories. As for the sports (rec.sport.hockey at 66% positive and rec.sport.baseball at 71%), as well as medical/science (sci.med at 71% and sci.space at 73%) categories, the data point out a presence of moderate positive sentiment with a substantial share of negative (29-34%). The political categories (talk.politics.guns at 70% positive and talk.politics.mideast at 78%) are somewhat ambiguous as to sentiment. Among these, talk.politics.guns has the highest negative sentiment (30%). The technical categories (comp.sys.ibm.pc.hardware at 80% and sci.crypt at 81%) are characterized by a predominance of positive tones. The data reveals that the majority of religious and commercial categories are in the positive trend, while topics of sports and politics show a greater presence of the negative sentiment."
      ],
      "metadata": {
        "id": "s4yt-16cM72H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**4. Evaluation**\n",
        "\n",
        "**4.1 Evaluate the Performance of Your Naive Bayes Classifier Using Metrics Such as Accuracy, Precision, Recall, and F1-Score**  \n",
        "\n",
        "The model performance was evaluated using various classification metrics available in scikit-learn.  "
      ],
      "metadata": {
        "id": "erTYywNINBPH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "# Calculate metrics\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "precision = precision_score(y_test, y_pred, average='weighted')\n",
        "recall = recall_score(y_test, y_pred, average='weighted')\n",
        "f1 = f1_score(y_test, y_pred, average='weighted')\n",
        "\n",
        "print(f\"Accuracy: {accuracy:.2f}\")\n",
        "print(f\"Precision: {precision:.2f}\")\n",
        "print(f\"Recall: {recall:.2f}\")\n",
        "print(f\"F1-Score: {f1:.2f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NgWkSZN6NGeR",
        "outputId": "9c7c713c-0f08-4be7-ff50-134761f35930"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.85\n",
            "Precision: 0.85\n",
            "Recall: 0.85\n",
            "F1-Score: 0.84\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The model made 85% as its accuracy, while precision, recall, and F1-score were equal to 86%, 85%, and 84%, respectively. The weighted metrics reflect the equal distribution of classes over the 20 categories, therefore indicating the uniform performance of the model. The little reduction in the F1-score suggests that there is a minor trade-off between precision and recall, which is most likely caused by those categories that are confused."
      ],
      "metadata": {
        "id": "UKPc1KN9NLbC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**4.2 Discuss the Performance of the Model and Any Challenges Encountered During the Classification Process**  \n",
        "\n",
        "The Naive Bayes classifier achieved an accuracy of 85% and adequately demonstrated its capability to use TF-IDF features to classify. The even distribution of the dataset (100 instances per category) probably had a positive effect on the classifier's stability. A few difficulties arose from the text data being noisy (e.g., headers and other metadata), though preprocessing has more than likely removed most of these contaminants, they might still influence the quality of the features. Some important words may have been left out due to the max_features=5000 restriction in TF-IDF and also the model's independence assumption that may lose the contextual characters of the text. The future work may include hyper-parameter setting trials or trying different models like SVM to get better results."
      ],
      "metadata": {
        "id": "r0FvvjcQNMRL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**4.3 Reflect on the Sentiment Analysis Results and Their Implications Regarding the Content of the Blog Posts**:\n",
        "\n",
        "Sentiment analysis by TextBlob shows 77.15% positive (1,543) and 22.85% negative (457) sentiments with no neutral classes. The high positive percentages of 'soc.religion.christian' (87%) and 'talk.religion.misc' (86%) give an idea of the nature of these categories being uplifting, while 'rec.sport.hockey' (66%) and 'sci.med' (71%) could be interpreted as having a slight positive trend with some negative aspects possibly due to competition or criticism. These categories reflect the presence of terms and ideas related to the discourse around these topics, which are mentioned in the last sentence of the text given. On the other hand, the likes of 'talk.politics.guns' (70% positive, 30% negative) show position arguments or a debate between opposing views. No neutral sentiment can be detected, which could be due to TextBlob being very sensitive to emotions or a preference for highly opinionated posts present in the dataset. Each of these points forms a timeline of blog content, with religious and technical themes leading the positive charge and political and sports topics dominating the negative mood, thus revealing user sentiment in the context of topics."
      ],
      "metadata": {
        "id": "zdOr6q2hNObI"
      }
    }
  ]
}